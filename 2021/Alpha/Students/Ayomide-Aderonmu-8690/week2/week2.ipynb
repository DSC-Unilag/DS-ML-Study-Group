{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 2 Assessment\n",
    "#### Scraping Brief\n",
    "`Website`:  [All products | Books to Scrape - Sandbox](http://books.toscrape.com/)\n",
    "\n",
    "`Detail`: Books to Scrape is a site built for the sole purpose of scraping practice. It contains a list of 1000 books.\n",
    "\n",
    "`Task`: Create a scraper that crawls through the website and scrapes details about all 1000 books. For each book, collect the:\n",
    "- Name\n",
    "- Image URL\n",
    "- Price\n",
    "- Rating\n",
    "\n",
    "These details are to be stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done scraping page 1 ... \n",
      "Done scraping page 2 ... \n",
      "Done scraping page 3 ... \n",
      "Done scraping page 4 ... \n",
      "Done scraping page 5 ... \n",
      "Done scraping page 6 ... \n",
      "Done scraping page 7 ... \n",
      "Done scraping page 8 ... \n",
      "Done scraping page 9 ... \n",
      "Done scraping page 10 ... \n",
      "Done scraping page 11 ... \n",
      "Done scraping page 12 ... \n",
      "Done scraping page 13 ... \n",
      "Done scraping page 14 ... \n",
      "Done scraping page 15 ... \n",
      "Done scraping page 16 ... \n",
      "Done scraping page 17 ... \n",
      "Done scraping page 18 ... \n",
      "Done scraping page 19 ... \n",
      "Done scraping page 20 ... \n",
      "Done scraping page 21 ... \n",
      "Done scraping page 22 ... \n",
      "Done scraping page 23 ... \n",
      "Done scraping page 24 ... \n",
      "Done scraping page 25 ... \n",
      "Done scraping page 26 ... \n",
      "Done scraping page 27 ... \n",
      "Done scraping page 28 ... \n",
      "Done scraping page 29 ... \n",
      "Done scraping page 30 ... \n",
      "Done scraping page 31 ... \n",
      "Done scraping page 32 ... \n",
      "Done scraping page 33 ... \n",
      "Done scraping page 34 ... \n",
      "Done scraping page 35 ... \n",
      "Done scraping page 36 ... \n",
      "Done scraping page 37 ... \n",
      "Done scraping page 38 ... \n",
      "Done scraping page 39 ... \n",
      "Done scraping page 40 ... \n",
      "Done scraping page 41 ... \n",
      "Done scraping page 42 ... \n",
      "Done scraping page 43 ... \n",
      "Done scraping page 44 ... \n",
      "Done scraping page 45 ... \n",
      "Done scraping page 46 ... \n",
      "Done scraping page 47 ... \n",
      "Done scraping page 48 ... \n",
      "Done scraping page 49 ... \n",
      "Done scraping the last page!\n"
     ]
    }
   ],
   "source": [
    "# initialise necessary variables\n",
    "titles = []\n",
    "prices = []\n",
    "ratings = []\n",
    "imageURLs = []\n",
    "page = ''\n",
    "page_count = 1\n",
    "\n",
    "\n",
    "# crawl through all pages\n",
    "while True:    \n",
    "    base_site = f'http://books.toscrape.com/{page}'\n",
    "    \n",
    "    # Making a get request\n",
    "    response = requests.get(base_site)\n",
    "\n",
    "    # Extracting the HTML\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    parent = soup.find_all('article')\n",
    "    titles.extend([title.find_all('a')[1].get('title') for title in parent])                # extract book title\n",
    "    prices.extend([price.text for price in soup.select('.price_color')])                    # extract prices\n",
    "    ratings.extend([rating.attrs['class'][1] for rating in soup.select('.star-rating')])    # extract rating\n",
    "    # or: ratings.extend([rating.get('class')[1] for rating in soup.select('.star-rating')])    # extract rating\n",
    "    imageURLs.extend([image.find_all('a')[0].find('img').get('src') for image in parent])   # extact imageURL\n",
    "    \n",
    "    \n",
    "    # if there next page button, extract the relative url; else end the scraping process\n",
    "    if soup.select('.next'):\n",
    "        page = 'catalogue/{}'.format(soup.select('.next')[0].select('a')[0].get('href').split('/')[-1])\n",
    "        print('Done scraping page {} ... '.format(page_count))\n",
    "        page_count += 1\n",
    "    else:\n",
    "        print('Done scraping the last page!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4)"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>ImageURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>Three</td>\n",
       "      <td>media/cache/2c/da/2cdad67c44b002e7ead0cc35693c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>One</td>\n",
       "      <td>media/cache/26/0c/260c6ae16bce31c8f8c95daddd9f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>One</td>\n",
       "      <td>media/cache/3e/ef/3eef99c9d9adef34639f51066202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>Four</td>\n",
       "      <td>media/cache/32/51/3251cf3a3412f53f339e42cac213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>Five</td>\n",
       "      <td>media/cache/be/a5/bea5697f2534a2f86a3ef27b5a8c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Name   Price Rating  \\\n",
       "0                   A Light in the Attic  £51.77  Three   \n",
       "1                     Tipping the Velvet  £53.74    One   \n",
       "2                             Soumission  £50.10    One   \n",
       "3                          Sharp Objects  £47.82   Four   \n",
       "4  Sapiens: A Brief History of Humankind  £54.23   Five   \n",
       "\n",
       "                                            ImageURL  \n",
       "0  media/cache/2c/da/2cdad67c44b002e7ead0cc35693c...  \n",
       "1  media/cache/26/0c/260c6ae16bce31c8f8c95daddd9f...  \n",
       "2  media/cache/3e/ef/3eef99c9d9adef34639f51066202...  \n",
       "3  media/cache/32/51/3251cf3a3412f53f339e42cac213...  \n",
       "4  media/cache/be/a5/bea5697f2534a2f86a3ef27b5a8c...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books = pd.DataFrame({'Name': titles, \"Price\": prices, \"Rating\": ratings,'ImageURL': imageURLs})\n",
    "print(books.shape, end='')\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert rating from words to digits\n",
    "ratings = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5}\n",
    "books.Rating = books.Rating.map(ratings)\n",
    "\n",
    "# save to csv\n",
    "books.to_csv('dsc_web_scraping.csv', encoding='utf-8-sig', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`jottings:`\n",
    "<!-- # less dynamic:\n",
    "# titles = []\n",
    "# prices = []\n",
    "# ratings = []\n",
    "# imageURLs = []\n",
    "\n",
    "# # crawl through all 50 pages\n",
    "# for i in range(1, 51):\n",
    "#     base_site = f'http://books.toscrape.com/catalogue/page-{i}.html'.format(i)\n",
    "    \n",
    "#     # Making a get request\n",
    "#     response = requests.get(base_site)\n",
    "    \n",
    "#     # Extracting the HTML\n",
    "#     html = response.content\n",
    "#     soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "#     parent = soup.find_all('article')\n",
    "#     titles.extend([title.find_all('a')[1].get('title') for title in parent])                # extract book title\n",
    "#     prices.extend([price.text for price in soup.select('.price_color')])                    # extract prices\n",
    "#     ratings.extend([rating.attrs['class'][1] for rating in soup.select('.star-rating')])    # extract rating\n",
    "#     imageURLs.extend([image.find_all('a')[0].find('img').get('src') for image in parent])   # extact imageURL\n",
    "\n",
    "\n",
    "<!-- # setting encoding to utf-8-sig removes the 'Â' character that shows in prices column when saved to csv without the encoding \n",
    "\n",
    "# books.to_excel('dsc_web_scraping.xlsx', index=False)  # no Â character when you save to excel -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
